{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Objective:\n",
    "â€¢ Understand and practice principal component analysis using scikit learn.\n",
    "Questions:\n",
    "    \n",
    "1. Scikit learn comes with pre-loaded dataset, load the digits dataset \n",
    "from that collection and write a helper function to plot the image using matplotlib. \n",
    "[Hint: Explore datasets module from scikit learn]\n",
    "\n",
    "2. Make a train -test split with 20% of the data set aside for testing. \n",
    "Fit a logistic regression model and observe the accuracy.\n",
    "\n",
    "3. Using scikit learn perform a PCA transformation such that the transformed dataset can explain 95% of the variance \n",
    "in the original dataset. Find out the number of components in the projected subspace.\n",
    "[Hint: Refer to decomposition module of scikit learn]\n",
    "\n",
    "4. Transform the dataset and fit a logistic regression and observe the accuracy. \n",
    "Compare it with the previous model and comment on the accuracy.\n",
    "[Hint: Project both the train and test samples to the new subspace]\n",
    "\n",
    "5. Compute the confusion matrix and count the number of instances that has gone wrong. \n",
    "For each of the wrong sample, plot the digit along with predicted and original label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Scikit learn comes with pre-loaded dataset, load the digits dataset \n",
    "from that collection and write a helper function to plot the image using matplotlib. \n",
    "[Hint: Explore datasets module from scikit learn]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Create feature matrix\n",
    "X = digits.data\n",
    "\n",
    "# Create target vector\n",
    "y = digits.target\n",
    "\n",
    "# View the first observation's feature values\n",
    "X[0]\n",
    "\n",
    "\n",
    "print(digits.data.shape)\n",
    "print(digits.target.shape)\n",
    "#print(digits.COL_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optical Recognition of Handwritten Digits Data Set\n",
      "===================================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      "References\n",
      "----------\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(digits['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(digits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first observation's feature values as a matrix\n",
    "X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the first observation's feature values as an image\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[1]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. Make a train -test split with 20% of the data set aside for testing. \n",
    "Fit a logistic regression model and observe the accuracy.\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y, random_state = 10, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ln_model = LogisticRegression()\n",
    "\n",
    "ln_model.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = ln_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(predicted_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "#x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. Using scikit learn perform a PCA transformation such that the transformed dataset can explain 95% of the variance \n",
    "in the original dataset. Find out the number of components in the projected subspace.\n",
    "[Hint: Refer to decomposition module of scikit learn]\n",
    "\n",
    "https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components=2)\n",
    "pca = PCA()\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "\n",
    "#principalDf = pd.DataFrame(data = principalComponents\n",
    " #            , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. Transform the dataset and fit a logistic regression and observe the accuracy. \n",
    "Compare it with the previous model and comment on the accuracy.\n",
    "[Hint: Project both the train and test samples to the new subspace]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ln_model = LogisticRegression()\n",
    "\n",
    "ln_model.fit(x_train, y_train)\n",
    "\n",
    "#classifier.fit(X_train, y_train)\n",
    "y_pred = ln_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the Performance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy' + str(accuracy_score(y_test, y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first try to use 1 principal component to train our algorithm. To do so, execute the following code:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "#classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "#y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "ln_model = LogisticRegression()\n",
    "\n",
    "ln_model.fit(x_train, y_train)\n",
    "\n",
    "#classifier.fit(X_train, y_train)\n",
    "y_pred = ln_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the Performance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy' + str(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first try to use 3 principal component to train our algorithm. To do so, execute the following code:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "#classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "#y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "ln_model = LogisticRegression()\n",
    "\n",
    "ln_model.fit(x_train, y_train)\n",
    "\n",
    "#classifier.fit(X_train, y_train)\n",
    "y_pred = ln_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35  0  0  0  1  0  1  0  0  0]\n",
      " [ 0 23  4  0  0  1  1  0  3  2]\n",
      " [ 0  3 27  2  0  0  0  0  1  1]\n",
      " [ 0  0  2 32  0  0  0  2  0  4]\n",
      " [ 0  1  0  0 31  1  0  0  1  0]\n",
      " [ 0  0  0  0  0 26  0  1  2  3]\n",
      " [ 0  1  0  0  0  0 36  0  0  0]\n",
      " [ 0  0  0  0  2  0  0 35  3  0]\n",
      " [ 0  5  2  0  0  3  2  2 16  3]\n",
      " [ 0  0  1  3  0  2  0  6  0 27]]\n",
      "Accuracy0.8\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the Performance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy' + str(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('Accent', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. Transform the dataset and fit a logistic regression and observe the accuracy. \n",
    "Compare it with the previous model and comment on the accuracy.\n",
    "[Hint: Project both the train and test samples to the new subspace]\n",
    "'''\n",
    "\n",
    "projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. Compute the confusion matrix and count the number of instances that has gone wrong. \n",
    "For each of the wrong sample, plot the digit along with predicted and original label.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    " \n",
    "digits = datasets.load_digits()\n",
    " \n",
    "X = digits.data\n",
    "y = digits.target\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " \n",
    "logisticRegr = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "logisticRegr.predict(X_test[0].reshape(1,-1))\n",
    " \n",
    "logisticRegr.predict(X_test[0:10])\n",
    "predictions = logisticRegr.predict(X_test)\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    " \n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    " \n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = f'Accuracy Score: {score:.2f}'\n",
    "plt.title(all_sample_title, size = 12)\n",
    "plt.show()\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
